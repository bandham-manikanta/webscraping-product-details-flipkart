{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "import pandas as pd\n",
    "import re\n",
    "import os\n",
    "import time\n",
    "from urllib.parse import urlparse\n",
    "import random\n",
    "from multiprocessing.pool import ThreadPool\n",
    "from IPython.display import clear_output\n",
    "\n",
    "all_txt_files = glob.glob(os.getcwd()+'\\\\prod_urls\\\\*.txt')\n",
    "driver = None\n",
    "dumb_urls = ['https://www.thepythoncode.com/','https://www.geeksforgeeks.org/','https://pynative.com/','http://facebook.com/','https://www.amazon.in/','https://www.tutorialspoint.com/','https://docs.python.org/','https://chromedriver.chromium.org/','https://github.com/','https://twitter.com/']\n",
    "\n",
    "home_dir = os.getcwd()\n",
    "results_dir = home_dir + '\\\\results'\n",
    "images_dir = results_dir + '\\\\images'\n",
    "\n",
    "all_dataframes = dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['E:\\\\projects-ds-ws\\\\webscraping-product-details-flipkart\\\\prod_urls\\\\Arts, Language and Linguistic Books.txt']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prod_url_txt_files = [txt_file for txt_file in glob.glob(home_dir+'\\\\prod_urls\\\\Arts, Language and Linguistic Books.txt')]\n",
    "\n",
    "genres_and_links = dict()\n",
    "for filename in prod_url_txt_files:\n",
    "    with open(filename,'r') as file:\n",
    "        relative_filename = filename.split('\\\\')[-1]\n",
    "        genres_and_links[relative_filename.replace('.txt','')] = file.readlines()\n",
    "prod_url_txt_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000 - Arts, Language and Linguistic Books\n"
     ]
    }
   ],
   "source": [
    "for kk,vv in genres_and_links.items():\n",
    "    print(len(vv),'-',kk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_multispaces_with_single_space(text):\n",
    "    text = re.sub('\\\\s+',' ',text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_image(soup,main_counter,genre,link,driver,existing_images_list):\n",
    "    image_urls = set()\n",
    "    image_url = None\n",
    "    i_names = list()\n",
    "\n",
    "    i_file_name = results_dir + '\\\\images\\\\' + genre + '\\\\' +str(main_counter)+'.jpeg'\n",
    "    if i_file_name not in existing_images_list:\n",
    "        #driver.get(dumb_urls[random.randint(0,len(dumb_urls)-1)])\n",
    "        #driver.get(link)\n",
    "        #page_source = driver.page_source\n",
    "        #soup = BeautifulSoup(page_source,'html.parser')\n",
    "        image_div_ele = soup.find('div',{'class':'_1ov7-N'})\n",
    "        if image_div_ele:\n",
    "            img = image_div_ele.find('img')\n",
    "            if img:\n",
    "                if img.has_attr('src'):\n",
    "                    image_url = img.get('src')\n",
    "        if image_url is not None:\n",
    "            image_url = re.sub('\\\\w+?q=\\\\d+','',image_url)\n",
    "            image_url = re.sub('https://rukminim1.flixcart.com/image/\\\\d+/\\\\d+/','https://rukminim1.flixcart.com/image/512/512/',image_url)\n",
    "            url_parse_results = urlparse(image_url)\n",
    "            if all([url_parse_results.scheme, url_parse_results.netloc, url_parse_results.path]):\n",
    "                image_urls.add(image_url)\n",
    "    else:\n",
    "        i_name = i_file_name.split('\\\\')[-1]\n",
    "        i_names.append(i_name)\n",
    "        return i_names;\n",
    "\n",
    "    if len(image_urls)==0:\n",
    "        image_url = 'https://img1a.flixcart.com/www/linchpin/fk-cp-zion/img/flipkart-plus_4ee2f9.png'\n",
    "        image_urls.add(image_url)\n",
    "            \n",
    "    for i_url in image_urls:\n",
    "        if i_file_name not in existing_images_list:\n",
    "            requests.get(dumb_urls[random.randint(0,len(dumb_urls)-1)])\n",
    "            i_response = requests.get(i_url)\n",
    "            with open(i_file_name, 'wb') as file:\n",
    "                file.write(i_response.content)\n",
    "        i_name = i_file_name.split('\\\\')[-1]\n",
    "        i_names.append(i_name)\n",
    "        break;\n",
    "    return i_names;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_data(soup,link,dumb_urls):\n",
    "    title = 'NA'\n",
    "    price = 'NA'\n",
    "    publisher = 'NA'\n",
    "    no_of_pages = 'NA'\n",
    "    isbn = 'NA'\n",
    "    language = 'NA'\n",
    "    rating = 'NA'\n",
    "    author = 'NA'\n",
    "    summary = 'NA'\n",
    "    \n",
    "    title_ele = soup.find('span',{'class':'_35KyD6'})\n",
    "    title = title_ele.get_text()\n",
    "    title = replace_multispaces_with_single_space(title)\n",
    "        \n",
    "    price_ele = soup.find('div',{'class':'_1vC4OE _3qQ9m1'})\n",
    "    if price_ele:\n",
    "        price = price_ele.get_text()\n",
    "        price = re.sub('â‚¹','',price)\n",
    "            \n",
    "    spec_tbody_elements = soup.find_all('tr',{'class':'_3_6Uyw row'})\n",
    "    for tbody in spec_tbody_elements:\n",
    "        tr_elements = tbody.find_all('td')\n",
    "        if len(tr_elements)==2:\n",
    "            if tr_elements[0] and tr_elements[0].text=='Imprint':\n",
    "                publisher = tr_elements[1].get_text()\n",
    "    \n",
    "    highlights_li_elements = soup.find_all('li',{'class':'_2-riNZ'})\n",
    "    for li in highlights_li_elements:\n",
    "        if li.get_text() and li.get_text().startswith('Pages'):\n",
    "            no_of_pages = li.get_text().split(': ')[1]\n",
    "        if li.get_text() and li.get_text().startswith('ISBN'):\n",
    "            isbn = li.get_text().split(': ')[1]\n",
    "        if li.get_text() and li.get_text().startswith('Language'):\n",
    "            language = li.get_text().split(': ')[1]\n",
    "        \n",
    "    rating_parent_element = soup.find('div',{'class':'_3ors59'})\n",
    "    if rating_parent_element:\n",
    "        rating_element = rating_parent_element.find('div',{'class':'hGSR34'})\n",
    "        if rating_element:\n",
    "            rating = rating_element.get_text()\n",
    "        \n",
    "    book_desc_list = list()\n",
    "    author_description_elements = soup.find_all('div',{'class':'_1y9a40'})\n",
    "    for element in author_description_elements:\n",
    "        sub_ele = element.find('div',{'class':'_1oCqc9'})\n",
    "        if sub_ele and sub_ele.get_text()=='Author':\n",
    "            author = element.find('div',{'class':'_3cpW1u'}).get_text().strip()\n",
    "        if sub_ele and sub_ele.get_text()=='Description':\n",
    "            sub_sub_ele = element.find('div',{'class':'_3cpW1u'})\n",
    "            if sub_sub_ele:\n",
    "                summary = sub_sub_ele.get_text()\n",
    "                summary = replace_multispaces_with_single_space(summary)\n",
    "\n",
    "    return (title,price,publisher,no_of_pages,isbn,language,rating,author,summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arts, Language and Linguistic Books\n",
      "300\n",
      "301\n",
      "302\n",
      "303\n",
      "304\n"
     ]
    }
   ],
   "source": [
    "driver = None\n",
    "\n",
    "def scrap_data(links,driver,genre):\n",
    "\n",
    "    titles = list()\n",
    "    prices = list()\n",
    "    publishers = list()\n",
    "    list_of_no_of_pages = list()\n",
    "    isbns = list()\n",
    "    genres = list()\n",
    "    ratings = list()\n",
    "    authors = list()\n",
    "    summaries = list()\n",
    "    image_names = list()\n",
    "    languages= list()\n",
    "    \n",
    "    main_counter = 1\n",
    "    for link in links:\n",
    "        if driver is None:\n",
    "            #options = webdriver.ChromeOptions()\n",
    "            #options.add_argument(\"--start-maximized\")\n",
    "            #driver = webdriver.Chrome(executable_path=\"chromedriver.exe\",options=options)\n",
    "            driver = webdriver.Firefox(executable_path='geckodriver.exe')\n",
    "        \n",
    "        driver.get(dumb_urls[random.randint(0,len(dumb_urls)-1)])\n",
    "        driver.get(link)\n",
    "        page_source = driver.page_source\n",
    "        soup = BeautifulSoup(page_source,'html.parser')\n",
    "        \n",
    "        #async_image_results = pool1.apply_async(save_image, (soup,main_counter,genre,link,driver,all_image_list))\n",
    "        i_names = save_image(soup,main_counter,genre,link,driver,all_image_list)\n",
    "        #async_other_results = pool2.apply_async(collect_data, (soup,link,dumb_urls))\n",
    "        title,price,publisher,no_of_pages,isbn,language,rating,author,summary = collect_data(soup,link,dumb_urls)\n",
    "\n",
    "        #i_names = async_image_results.get()\n",
    "        #book_title,price,publisher,no_of_pages,isbn,language,rating,author,book_summary = async_other_results.get()\n",
    "                \n",
    "        titles.append(title)\n",
    "        prices.append(price)\n",
    "        publishers.append(publisher)\n",
    "        list_of_no_of_pages.append(no_of_pages)\n",
    "        isbns.append(isbn)\n",
    "        genres.append(genre.replace(' Books',''))\n",
    "        ratings.append(rating)\n",
    "        authors.append(author)\n",
    "        summaries.append(summary)\n",
    "        image_names.append(', '.join(i_names))\n",
    "        languages.append(language)\n",
    "        \n",
    "        print(main_counter)\n",
    "        main_counter += 1\n",
    "\n",
    "        if main_counter%100==0:\n",
    "        #if False:\n",
    "            clear_output()\n",
    "            print(genre)\n",
    "            temp_dict = dict()\n",
    "            temp_dict['title'] = titles\n",
    "            temp_dict['price'] = prices\n",
    "            temp_dict['publishers'] = publishers\n",
    "            temp_dict['number_of_pages'] = list_of_no_of_pages\n",
    "            temp_dict['isbn_10'] = isbns\n",
    "            temp_dict['isbn_13'] = isbns\n",
    "            temp_dict['genres'] = genres\n",
    "            temp_dict['rating'] = ratings\n",
    "            temp_dict['authors'] = authors\n",
    "            temp_dict['description'] = summaries\n",
    "            temp_dict['image'] = image_names\n",
    "            temp_dict['thumb'] = image_names\n",
    "            temp_dict['languages'] = languages\n",
    "            temp_df = pd.DataFrame(temp_dict)\n",
    "            temp_df = temp_df[['isbn_10','isbn_13','image','thumb','title','authors','languages','genres','number_of_pages','description','publishers','rating','price']]\n",
    "            export_file_name = results_dir + '\\\\' + genre+'_'+str(main_counter) + '.csv'\n",
    "            temp_df.to_csv(export_file_name,index=False)\n",
    "\n",
    "    return titles,prices,publishers,list_of_no_of_pages,isbns,genres,ratings,authors,summaries,image_names,languages\n",
    "\n",
    "for genre, links in genres_and_links.items():\n",
    "    \n",
    "    results_dict = dict()\n",
    "    titles,prices,publishers,list_of_no_of_pages,isbns,genres,ratings,authors,summaries,image_names,languages = None,None,None,None,None,None,None,None,None,None,None\n",
    "\n",
    "    #create images folder for each genre\n",
    "    genre_images_dir = results_dir + '\\\\images\\\\' + genre\n",
    "    if not os.path.exists(genre_images_dir):\n",
    "        os.makedirs(genre_images_dir)\n",
    "    all_image_list = glob.glob(genre_images_dir+'\\\\*.jpeg')\n",
    "    \n",
    "    pool = ThreadPool(processes=7)\n",
    "    #pool2 = ThreadPool(processes=2)\n",
    "    \n",
    "    async_resp = pool.apply_async(scrap_data,(links,driver,genre))\n",
    "\n",
    "    (titles,prices,publishers,list_of_no_of_pages,isbns,genres,ratings,authors,summaries,image_names,languages) = async_resp.get()\n",
    "\n",
    "    results_dict['title'] = titles\n",
    "    results_dict['price'] = prices\n",
    "    results_dict['publishers'] = publishers\n",
    "    results_dict['number_of_pages'] = list_of_no_of_pages\n",
    "    results_dict['isbn_10'] = isbns\n",
    "    results_dict['isbn_13'] = isbns\n",
    "    results_dict['genres'] = genres\n",
    "    results_dict['rating'] = ratings\n",
    "    results_dict['authors'] = authors\n",
    "    results_dict['description'] = summaries\n",
    "    results_dict['image'] = image_names\n",
    "    results_dict['thumb'] = image_names\n",
    "    results_dict['languages'] = languages\n",
    "    \n",
    "    df = pd.DataFrame(results_dict)\n",
    "    df = df[['isbn_10','isbn_13','image','thumb','title','authors','languages','genres','number_of_pages','description','publishers','rating','price']]\n",
    "    export_file_name = results_dir + '\\\\' + genre + '.csv'\n",
    "    df.to_csv(export_file_name,index=False)\n",
    "    ultimate_dataframes[genre] = df\n",
    "    print('yahooooooo!!! Completed for','\\\"',genre,'\\\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if driver is not None:\n",
    "    driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Everything completed. Thank god.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
